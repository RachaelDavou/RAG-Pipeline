{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wikipedia\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  \n",
    "from langchain_core.documents import Document  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API key - replace with your key\n",
    "OPENAI_API_KEY = 'your-openai-api-key-here'\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load Wikipedia documents based on a list of topics\n",
    "def load_wikipedia_docs(topics):\n",
    "    documents = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        try:\n",
    "            page = wikipedia.page(topic, auto_suggest=False)\n",
    "            doc = Document(\n",
    "                page_content=page.content,\n",
    "                metadata={\"source\": page.url, \"title\": page.title}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            print(f\"Loaded: {page.title}\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            page = wikipedia.page(e.options[0], auto_suggest=False)\n",
    "            doc = Document(\n",
    "                page_content=page.content,\n",
    "                metadata={\"source\": page.url, \"title\": page.title}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            print(f\"Loaded: {page.title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {topic}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia topics on health and medicine\n",
    "wiki_topics = [\n",
    "    \"Vaccination\",\n",
    "    \"Antibiotic\",\n",
    "    \"Vitamin D\",\n",
    "    \"Heart disease\",\n",
    "    \"Immune system\",\n",
    "    \"Covid-19\",\n",
    "    \"Cancer\",\n",
    "    \"Tuberculosis\",\n",
    "    \"Obesity\",\n",
    "    \"Cholesterol\",\n",
    "    \"Stroke\",\n",
    "    \"Arthritis\"\n",
    "]\n",
    "\n",
    "print(\"Loading Wikipedia articles...\\n\")\n",
    "wiki_docs = load_wikipedia_docs(wiki_topics)\n",
    "print(f\"\\nLoaded {len(wiki_docs)} Wikipedia articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load documents from web URLs using LangChain's WebBaseLoader\n",
    "def load_web_docs(urls):\n",
    "    \"\"\"Load documents from web URLs using LangChain's WebBaseLoader\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            print(f\"Loaded: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {url}: {e}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health articles from the web\n",
    "web_urls = [\n",
    "    \"https://www.who.int/news-room/fact-sheets/detail/malaria\",\n",
    "    \"https://www.who.int/news-room/fact-sheets/detail/diabetes\",\n",
    "    \"https://www.who.int/news-room/fact-sheets/detail/hypertension\",\n",
    "    \"https://www.who.int/health-topics/hepatitis#tab=tab_1\",\n",
    "    \"https://www.who.int/health-topics/ebola#tab=tab_1\",\n",
    "    \"https://www.who.int/health-topics/nutrition#tab=tab_1\",\n",
    "    \"https://www.who.int/health-topics/physical-activity#tab=tab_1\",\n",
    "    \"https://www.who.int/health-topics/self-care#tab=tab_1\"\n",
    "    \n",
    "]\n",
    "\n",
    "print(\"Loading web articles...\\n\")\n",
    "web_docs = load_web_docs(web_urls)\n",
    "print(f\"\\nLoaded {len(web_docs)} web articles\")\n",
    "\n",
    "# Combine all documents\n",
    "all_documents = wiki_docs + web_docs\n",
    "print(f\"Total documents loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into smaller chunks of 500 characters for better embedding and retrieval \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       \n",
    "    chunk_overlap=100,    \n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split on paragraphs first, then sentences to preserve context\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "print(f\"Split {len(all_documents)} documents into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings and Building the FAISS Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extracting text content from chunks\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# Creating embeddings \n",
    "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=False)\n",
    "print(f\"Created {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve relevant chunks based on a query\n",
    "def retrieve(query, k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Get the relevant chunks\n",
    "    retrieved_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        retrieved_chunks.append({\n",
    "            \"content\": chunks[idx].page_content,\n",
    "            \"source\": chunks[idx].metadata.get(\"source\", \"Unknown\"),\n",
    "            \"distance\": distances[0][i]\n",
    "        })\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "# Function to enhance the prompt with the query and retrieved context\n",
    "def enhance_prompt(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([chunk[\"content\"] for chunk in retrieved_chunks])\n",
    "    prompt = f\"\"\"Use the following context to answer the question and if the context doesn't contain enough information to answer, say so.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "QUESTION: {query}\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to generate an answer using OpenAI\n",
    "def generate(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful medical information assistant. Answer questions based only on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Main RAG query function\n",
    "def rag_query(question, k=3, show_sources=True):\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Retrieve the relevant chunks\n",
    "    retrieved = retrieve(question, k=k)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(\"Retrieved Sources:\")\n",
    "        for i, chunk in enumerate(retrieved, 1):\n",
    "            print(f\"  [{i}] {chunk['source'][:80]}... (distance: {chunk['distance']:.3f})\")\n",
    "        print()\n",
    "    \n",
    "    # Augment the prompt with context\n",
    "    enhanced_prompt = enhance_prompt(question, retrieved)\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    answer = generate(enhanced_prompt)\n",
    "    \n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions from the wiki topics and web link topics for testing the RAG system\n",
    "test_questions = [\n",
    "    \"Why is vaccination important for public health?\",\n",
    "    \"What are the common side effects of antibiotics?\",\n",
    "    \"How does vitamin D deficiency affect the body?\",\n",
    "    \"What are the major risk factors associated with heart disease?\",\n",
    "    \"How does Covidâ€‘19 primarily spread between people?\",\n",
    "    \"What are the main symptoms of malaria?\",\n",
    "    \"What is the recommended diet for preventing type 2 diabetes?\",\n",
    "    \"How can high blood pressure be controlled through lifestyle?\",\n",
    "    \"What are the typical signs of hepatitis infection?\",\n",
    "    \"Why is regular physical activity beneficial for overall health?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TESTING THE RAG SYSTEM - HEALTH & MEDICINE Q&A\")\n",
    "print()\n",
    "\n",
    "for question in test_questions:\n",
    "    rag_query(question, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
